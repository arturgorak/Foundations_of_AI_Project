# -*- coding: utf-8 -*-
"""projekt_australia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HOEFd4w3_wYEcDNHuvVFUrxHVPCelyvb

## 1 Importy i pobranie danych
"""

# Numpy
import numpy as np

# Pandas
import pandas as pd

# Reprezentacja graficzna
import seaborn as sns
import matplotlib.pyplot as plt

# Pipeline
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion


# Uczenie płytkie
from sklearn.model_selection import train_test_split
from collections import Counter

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import roc_auc_score
from sklearn import  metrics


# StandardScaler 
from sklearn.preprocessing import StandardScaler

# inne
from scipy.stats.distributions import uniform, randint
import math

# Uczenie głębokie
from tensorflow import keras
import tensorflow as tf
from keras.callbacks import EarlyStopping
from keras import layers
from scipy.stats import reciprocal
from sklearn.model_selection import RandomizedSearchCV
from keras.layers import BatchNormalization
from keras.models import Sequential
from keras.layers import Flatten
from keras.layers import Activation
from keras.layers import Dropout
from keras.layers import Dense

data = pd.read_csv("data.csv")

data.head(5)

"""*   Data - The date of observation
*   Location - The common name of the location of the weather station
*   Rainfall - The amount of rainfall recorded for the day in mm
*   Evaporation - The so-called Class A pan evaporation (mm) in the 24 hours to 9am
*   Sunshine - The number of hours of bright sunshine in the day.
*   WindGustDir - The direction of the strongest wind gust in the 24 hours to midnight
*   WindGustSpeed - The speed (km/h) of the strongest wind gust in the 24 hours to midnight
*   WindDir9am - Direction of the wind at 9am
*   WindDir3pm - Direction of the wind at 3pm
*   WindSpeed9am- Wind speed (km/hr) averaged over 10 minutes prior to 9am
*   WindSpeed3pm -Wind speed (km/hr) averaged over 10 minutes prior to 3pm
*   Humidity9am - Humidity (percent) at 9am
*   Humidity3pm - Humidity (percent) at 3pm
*   Pressure9am - Atmospheric pressure (hpa) reduced to mean sea level at 9am
*   Pressure3pm - Atmospheric pressure (hpa) reduced to mean sea level at 3pm
*   Cloud9am - Fraction of sky obscured by cloud at 9am. This is measured in "oktas", which are a unit of eigths. It records how many
*   Cloud3pm - Fraction of sky obscured by cloud (in "oktas": eighths) at 3pm. See Cload9am for a description of the values
*   Temp9am - Temperature (degrees C) at 9am
*   Temp3pm - Temperature (degrees C) at 3pm
*   RainToday - Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0
*   RainTomorrow
The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the "risk".

## 2 Przygotowanie danych
Na początku sprawdźmy ile i gdzie są wartości nullowe
"""

# ile i gdzie wartości nullowe
data.isnull().sum()

len(data)

"""Prawie 6% wartości WindDir9am jest nullowych i około 1.5 % WindDir3pm oraz Location. Trzeba będzie czymś je uzupełnić. W pozostałych kolumnach brakuących wartości jest < 1%"""

data.info()

data.describe()

"""Na początku zamieńmy wszystkie wartości yes, no na 1 i 0"""

data = data.replace(["Yes", "No"], [1, 0], regex = True)
data.head()

"""Sprawdźmy korelacje pomiędzy kolumnami aby nie było powielania informacji


"""

corr = data.corr()
corr

plt.figure(figsize=(16, 6))

mask = np.triu(np.ones_like(data.corr(), dtype=np.bool))

heatmap = sns.heatmap(data.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')

heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);

sns.pairplot( data=data, vars=('Pressure9am','Pressure3pm', 'Temp9am', 'Temp3pm', 'Evaporation', 'Sunshine'), hue='RainTomorrow' )

"""Zdefiniujmy funkcje która będzie dziliła nam dane na biny a nastepnie dla każdego bina wyliczała prawdopodobieństwo opadu"""

def check_probability(input_name, size=10):
  new_name = input_name + " rounded values"
  data[new_name] = data[input_name].apply(lambda x: math.ceil(x))

  g = sns.factorplot(x=new_name,y="RainTomorrow",data=data, kind="bar", size = size)
  g.despine(left=True)
  g = g.set_ylabels("rain probability")

  data.drop(labels = [new_name], axis = 1, inplace = True)

"""### 2.1. Date
Spróbujmy wyciągnąć z daty miesiąc, po czym zamienić go na dane kategoryczne
"""

data['Month'] = pd.DatetimeIndex(data['Date']).month.astype(str)

data.head(5)

data.drop(labels = ["Date"], axis = 1, inplace = True)

data.columns

"""Sprawdźmy czy miesiąć ma wpływ na to czy następnego dnia będzie padać"""

g = sns.factorplot(x="Month",y="RainTomorrow",data=data, kind="bar", size = 6)
g.despine(left=True)
g = g.set_ylabels("rain probability")

"""Jak widać w miesiącach jesiennych możemy się liczyć z mniejszym prawdopodobieństwem opadu

### 2.2. Location
"""

data['Location'].value_counts()

g = sns.factorplot(x="Location",y="RainTomorrow",data=data, kind="bar", size = 10)
g.despine(left=True)
g = g.set_ylabels("rain probability")

"""Jak widać największe szanse na opad są w CoffsHarbour oraz WitchIsland

### 2.3. Rainfall i RainToday
Pomiędzy Rainfall i RainToday jest zależność polegająca na tym, że jeśli w Rainfall jest wartość większa od 1 to RainToday jest True. W przeciwnym wypadku False. Także aby nie powielać informacji usuńmy kolumne RainToday
"""

data.drop(labels = ["RainToday"], axis = 1, inplace = True)

"""Jak możemy dostrzec Rainfall przyjmuje jakąś wartość kiedy w RainToday mamy 1, więć aby nie powielac informacji usuiemy kolumne RainToday i uzupełnijmy Rainfall średnią"""

g = sns.FacetGrid(data, col='RainTomorrow')
g = g.map(sns.distplot, "Rainfall")

data['Rainfall'].fillna(data['Rainfall'].mean(), inplace=True)

data['Rainfall'].isnull().sum()

"""### 2.4. Evaporation """

data['Evaporation'].isnull().sum()

check_probability("Evaporation")

"""Można powiedzieć, że im większe odparowanie tym mniejsza szansa na deszcz

### 2.5. Sunshine
"""

out = pd.cut(data["Sunshine"], bins=[x for x in range (0, 20)], include_lowest=True)
ax = out.value_counts(sort=False).plot.bar(rot=0, color="b", figsize=(20,6))
plt.show()

check_probability("Sunshine")

"""Jak widać powyżej im więcej słonecznych godzin tym mniejsza szansa na deszcz jutro"""

data['Sunshine'].isnull().sum()

"""### 2.6. WindDir
Będziemy musieli uzupełnić brakujące wartości poprzez danie w miejsce nulli najczęściej występującej wartości
"""

data['WindDir9am'].isnull().sum()

data['WindDir9am'].value_counts()

#data['WindDir9am'] = data['WindDir9am'].fillna(data['WindDir9am'].mode().iloc[0])

data['WindDir9am'].isnull().sum()

data['WindDir9am'].value_counts()

#data = pd.get_dummies(data, columns = ['WindDir9am'])

g = sns.factorplot(x="WindDir9am",y="RainTomorrow",data=data, kind="bar", size = 10)
g.despine(left=True)
g = g.set_ylabels("rain probability")

"""Największe szanse na deszcz są przy wietrze wiejącym wkierunku SSW (południowo zachodnim, ale bardziej skierowanemu na południe). Za chwilę zobaczymy czy podobne obserwacje będą przy wietsze o godzinie 15.00"""

data['WindDir3pm'].isnull().sum()

data['WindDir3pm'].value_counts()

#data['WindDir3pm'] = data['WindDir3pm'].fillna(data['WindDir3pm'].mode().iloc[0])

data['WindDir3pm'].isnull().sum()

#data = pd.get_dummies(data, columns = ['WindDir3pm'])

g = sns.factorplot(x="WindDir3pm",y="RainTomorrow",data=data, kind="bar", size = 10)
g.despine(left=True)
g = g.set_ylabels("rain probability")

"""### 2.7. Unnamed: 0
Wyrzućmy tą nic nie wnoszącą kolumne
"""

data.drop(labels = ["Unnamed: 0"], axis = 1, inplace = True)

data.columns

"""### 2.8. WindSpeed
Zastąpmy brakujące wartości wartościami średnimi
"""

data['WindSpeed9am'].fillna(data['WindSpeed9am'].mean(), inplace=True)
data['WindSpeed3pm'].fillna(data['WindSpeed3pm'].mean(), inplace=True)

check_probability("WindSpeed9am")

check_probability("WindSpeed3pm")

"""Humidity9am       14
Humidity3pm       13
Pressure9am       23
Pressure3pm       16
Cloud9am           0
Cloud3pm           0

### 2.9. Humidity
"""

data['Humidity3pm'].fillna(data['Humidity3pm'].mean(), inplace=True)
data['Humidity9am'].fillna(data['Humidity9am'].mean(), inplace=True)

check_probability("Humidity9am", 14)

check_probability("Humidity3pm", 15)

"""Również można dostrzec zależność, że im większa wilgotność tym większa szansa na opad

### 2.10. Pressure
"""

data['Pressure3pm'].fillna(data['Pressure3pm'].mean(), inplace=True) 
data['Pressure9am'].fillna(data['Pressure9am'].mean(), inplace=True)

check_probability('Pressure9am', 15)

check_probability('Pressure3pm', 20)

"""### 2.11. Cloud"""

check_probability("Cloud9am")

check_probability("Cloud3pm")

"""W obu przypadkach możemy dojśc do wniosku, że im większa część nieba jest przysłonięta chmurami to tym większe prawdopodobieństwo opadów

## 3 Pipeline'y
"""

class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.attribute_names]

"""Pipeline dla numerycznych


"""

numerical_fields = ["Evaporation", "Rainfall", "Sunshine", "WindGustSpeed", 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']

num_pipeline = Pipeline([
        ("select_numeric", DataFrameSelector(numerical_fields)),
        ("imputer", SimpleImputer(strategy="median")),
        ('scaler', StandardScaler())
    ])

"""Pipeline dla kategorycznych"""

catbin = ["Location","WindGustDir" ,"WindDir9am", "WindDir3pm", "Month"]

cat_pipeline = Pipeline([
        ("select_cat", DataFrameSelector(catbin)),
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("cat_encoder", OneHotEncoder(sparse=False, handle_unknown = 'ignore')),
    ])

preprocess_pipeline = FeatureUnion(transformer_list=[
        ("num_pipeline", num_pipeline),
        ("cat_pipeline", cat_pipeline),
    ])

"""## 4 Podział danych na train i test"""

y = data["RainTomorrow"]
X = data.drop("RainTomorrow", axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

num_pipeline.fit_transform(X_train)

cat_pipeline.fit_transform(X_train)

seed=123
kfold = StratifiedKFold(n_splits=10)

"""# 5 Uczenie modeli

## 5.1 Uczenie płytkie

### 5.1.1 Decision Tree
"""

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('scaler',  StandardScaler()),
    ('classifier', DecisionTreeClassifier())
    ])

dt_param_grid = {
    'classifier__max_depth': range(2, 10),
    'classifier__max_features': range(2, 14),
    }

gsDT = GridSearchCV(pipe, dt_param_grid, cv=kfold, verbose=10)
gsDT.fit(X_train,y_train)
DT_best = gsDT.best_estimator_

"""### 5.1.2 Random Forest Classifier"""

# Random Forest Classifier

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', RandomForestClassifier())])



rf_param_grid = { 
    'classifier__n_estimators': [100, 300],
    'classifier__max_features': ['auto', 'sqrt', 'log2'],
    'classifier__max_depth': [3,5,7,9],
    'classifier__criterion': ['gini']
}

gsRFC = GridSearchCV(pipe, rf_param_grid, cv=kfold, verbose=10)
gsRFC.fit(X_train,y_train)
RFC_best = gsRFC.best_estimator_

print(gsRFC.best_score_)

"""### 5.1.3 Gradient Boosting"""

# Gradient boosting tunning

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', GradientBoostingClassifier())])


gb_param_grid = {'classifier__loss' : ["deviance"],
              'classifier__n_estimators' : [100,200],
              'classifier__learning_rate': [0.05, 0.1],
              'classifier__max_depth': [4, 8, 12],
              'classifier__max_features': [0.5, 0.3, 0.1] 
              }

gsGBC = GridSearchCV(pipe, gb_param_grid, cv=kfold, verbose=10)

gsGBC.fit(X_train,y_train)

GBC_best = gsGBC.best_estimator_

# Best score
gsGBC.best_score_

"""### 5.1.4 Logistic Regression"""

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', LogisticRegression())])

LR_param_grid = {
    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
     'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
}


gsLR = GridSearchCV(pipe ,param_grid = LR_param_grid, cv=kfold, verbose = 10)

gsLR.fit(X_train,y_train)

LR_best = gsLR.best_estimator_

gsLR.best_score_

"""### 5.1.5 XGBoost"""

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', XGBClassifier())])

param_distribution = {
    'classifier__max_depth': randint(3, 20),
    'classifier__learning_rate': uniform(0.001, 0.1-0.001),
    'classifier__n_estimators': randint(50, 600),
    'classifier__gamma': uniform(0,2),
    'classifier__colsample_bytree': uniform(0.5, 0.5),
    'classifier__subsample': uniform(0.5, 0.5),
    'classifier__min_child_weight': randint(1, 11)
}


gsXGB = RandomizedSearchCV(pipe, param_distribution, n_iter=32, verbose=10 )

gsXGB.fit(X_train, y_train)
XGB_best = gsXGB.best_estimator_

# Best score
gsXGB.best_score_

"""### 5.1.6 SVC linear"""

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', SVC(kernel="linear"))])

param_grid = {
          
            'classifier__gamma': [0.001, 0.1, 1, ],
            'classifier__C': [0.001, 0.1, 1,],
            'classifier__probability': [True]
}

gsSVC_linear = GridSearchCV(pipe, param_grid, cv=kfold, verbose=10)

gsSVC_linear.fit(X_train, y_train)

gsSVC_linear_best = gsSVC_linear.best_estimator_

# Best score
gsSVC_linear.best_score_

"""### 5.1.7 Ada Boost"""

DTC = DecisionTreeClassifier()

pipe = Pipeline([
    ('preprocessing', preprocess_pipeline),
    ('classifier', AdaBoostClassifier(DTC, random_state=7))])

ada_param_grid = {"classifier__base_estimator__criterion" : ["gini", "entropy"],
              "classifier__base_estimator__splitter" :   ["best", "random"],
              "classifier__algorithm" : ["SAMME","SAMME.R"],
              "classifier__n_estimators" :[1,2],
              "classifier__learning_rate":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}

gsAda = GridSearchCV(pipe ,param_grid = ada_param_grid, cv=kfold, verbose = 10)

gsAda.fit(X_train, y_train)

ada_best = gsAda.best_estimator_

"""### 5.1.9 Voting classifier (hard)"""

# Hard voting 

hard_voting_class = VotingClassifier(estimators=[ ('DT', DT_best),('RFC', RFC_best),
('GBC',GBC_best), ('XGB', XGB_best)], voting='hard')

hard_voting = hard_voting_class.fit(X_train, y_train)

"""### 5.1.10 Voitng classifier (soft)"""

# Soft voting


soft_voting_class = VotingClassifier(estimators=[ ('DT', DT_best),('XGBoost', RFC_best),
('GBC',GBC_best), ('XGB', XGB_best)], voting='soft')

soft_voting = soft_voting_class.fit(X_train, y_train)

"""### 5.1.11 Porównianie modeli płytkich"""

models = []
models.append(('DecisionTree', gsDT.best_estimator_))
models.append(('RandomForest', gsRFC.best_estimator_))
models.append(('GradientBoosting', gsGBC.best_estimator_))
models.append(('XGBoost', gsXGB.best_estimator_))
models.append(('LogisticRegression', gsLR.best_estimator_))
models.append(('SVC_linear', gsSVC_linear.best_estimator_))
models.append(('Ada Boost', gsAda.best_estimator_))
models.append(('Hard voting', hard_voting))
models.append(('Soft voting', soft_voting))


precision_score = []
recall_score = []
f1_score = []
accuracy_score = []
for name, model in models:
    print(name)
    print("precision_score: {}".format(metrics.precision_score(y_test, model.predict(X_test)) ))
    print("recall_score: {}".format( metrics.recall_score(y_test, model.predict(X_test)) ))
    print("f1_score: {}".format( metrics.f1_score(y_test, model.predict(X_test)) ))
    print("accuracy_score: {}".format( metrics.accuracy_score(y_test, model.predict(X_test)) ))
    precision_score.append(metrics.precision_score(y_test, model.predict(X_test)))
    recall_score.append(metrics.recall_score(y_test, model.predict(X_test)))
    f1_score.append( metrics.f1_score(y_test, model.predict(X_test)))
    accuracy_score.append(metrics.accuracy_score(y_test, model.predict(X_test)))

models2 = []
models2.append(('DecisionTree', gsDT.best_estimator_))
models2.append(('RandomForest', gsRFC.best_estimator_))
models2.append(('GradientBoosting', gsGBC.best_estimator_))
models2.append(('XGBoost', gsXGB.best_estimator_))
models2.append(('LogisticRegression', gsLR.best_estimator_))
models.append(('SVC_linear', gsSVC_linear.best_estimator_))
models2.append(('Ada Boost', gsAda.best_estimator_))
#models2.append(('Hard voting', hard_voting))
models2.append(('Soft voting', soft_voting))


for name, model in models2:
    print(name)
    print("roc_score: {}".format(roc_auc_score(y_test, model.predict(X_test))))
   
    # calculate the fpr and tpr for all thresholds of the classification
    probs = model.predict_proba(X_test)
    preds = probs[:,1]
    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
    roc_auc = metrics.auc(fpr, tpr)
 
    # method I: plt
    import matplotlib.pyplot as plt
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, label = '%s AUC = %0.10f' % (name, roc_auc))
    plt.legend(loc = 'lower right')
    plt.plot([-0.1, 1.1], [0, 1],'r--')
    plt.xlim([-0.1, 1.1])
    plt.ylim([-0.1, 1.1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

plt.rcParams["figure.figsize"] = (25,12)
plt.show()

d = {'precision_score': precision_score, 
     'recall_score': recall_score, 
     'f1_score': f1_score,
     'accuracy_score' : accuracy_score
    }
df = pd.DataFrame(data=d)
df.insert(loc=0, column='Method', value=['Decision tree', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'Logistic Regression','SVC Linear', 'Ada Boost','Hard Voting', 'Soft Voting'])
df

"""## 5.2 Uczenie głębokie

Najpierw przetwórzmy nasze dane przy pomocy pipelineów
"""

X_train2 = preprocess_pipeline.transform(X_train)
X_test2 = preprocess_pipeline.transform(X_test)

"""### 5.2.1 Sieć bez bajerów"""

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model1 = keras.models.Sequential()
model1.add(layers.Dense(82, input_dim=82, activation='sigmoid'))
model1.add(layers.Dense(41, input_dim=82, activation='sigmoid'))
model1.add(layers.Dense(20, input_dim=82, activation='sigmoid'))
model1.add(layers.Dense(1, activation='sigmoid'))

model1.compile(loss="binary_crossentropy",
              optimizer="Adam",
              metrics=["accuracy"])

history1 = model1.fit(X_train2, y_train, epochs=100, validation_data=(X_test2, y_test))

pd.DataFrame(history1.history).plot(figsize=(25, 12))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

"""### 5.2.2 Sieć z early stoppingiem"""

model2 = keras.models.Sequential()
model2.add(layers.Dense(82, input_dim=82, activation='elu'))
model2.add(layers.Dense(41, input_dim=82, activation='elu'))
model2.add(layers.Dense(20, input_dim=82, activation='elu'))
model2.add(layers.Dense(1, activation='sigmoid'))

model2.compile(loss="binary_crossentropy",
              optimizer="Adam",
              metrics=["accuracy"])


early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)
history2 = model2.fit(X_train2, y_train, epochs=100, validation_data=(X_test2, y_test), callbacks=[early_stopping])

pd.DataFrame(history2.history).plot(figsize=(25, 12))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

"""### 5.2.3 Sieć z BatchNormalization"""

model3 = Sequential()
model3.add(Flatten(input_shape=[82,]))
model3.add(BatchNormalization())
model3.add(Activation('sigmoid'))

model3.add(Dense(300))
model3.add(BatchNormalization())
model3.add(Activation('sigmoid'))

model3.add(Dense(100))
model3.add(BatchNormalization())
model3.add(Activation('sigmoid'))

model3.add(Dense(1))
model3.add(Activation('sigmoid'))

model3.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer="sgd",
              metrics=["accuracy"])

history3 = model3.fit(X_train2, y_train, epochs=100, validation_data=(X_test2, y_test))

pd.DataFrame(history3.history).plot(figsize=(25, 12))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

"""### 5.2.4 Sieć z BatchNormalization i Dropoutem"""

model4 = Sequential()
model4.add(Flatten(input_shape=[82,]))
model4.add(BatchNormalization())
model4.add(Activation('elu'))
model4.add(Dropout(0.3))

model4.add(Dense(300))
model4.add(BatchNormalization())
model4.add(Activation('elu'))
model4.add(Dropout(0.3))

model4.add(Dense(100))
model4.add(BatchNormalization())
model4.add(Activation('elu'))
model4.add(Dropout(0.3))

model4.add(Dense(1))
model4.add(Activation('sigmoid'))

model4.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer="sgd",
              metrics=["accuracy"])

history4 = model4.fit(X_train2, y_train, epochs=100, validation_data=(X_test2, y_test))

pd.DataFrame(history4.history).plot(figsize=(25, 12))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

"""### 5.2.5 Randomized Search"""

keras.backend.clear_session()

def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[82,], activation_function='sigmoid'):
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation=activation_function))
    model.add(keras.layers.Dense(1))
    
    optimizer = keras.optimizers.SGD(lr=learning_rate)
    model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])
    return model

keras_class = tf.keras.wrappers.scikit_learn.KerasClassifier(build_model)
keras_class

param_distribs = {
    "n_hidden": [2, 3, 4, 5],
    "n_neurons": np.arange(1, 100), 
    "activation_function": ['relu', 'elu', 'sigmoid']
}

rnd_search_cv = RandomizedSearchCV(keras_class, param_distribs, n_iter=10, cv=3, verbose=2)
rnd_search_cv.fit(X_train2, y_train, epochs=100, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=20, mode='min', verbose=1)])

rnd_search_cv.best_params_



"""### 5.2.6. Porównanie modeli głębokich"""

plt.plot(pd.DataFrame(history1.history['val_accuracy']), label='Sieć bez bajerów')
plt.plot(pd.DataFrame(history2.history['val_accuracy']), label='Sieć z earlystoppingiem')
plt.plot(pd.DataFrame(history3.history['val_accuracy']), label='Sieć z BatchNormalization')
plt.plot(pd.DataFrame(history4.history['val_accuracy']), label='Sieć z BatchNormalizastion i Dropoutem')
plt.legend()
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.rcParams["figure.figsize"] = (25,12)
plt.show()